{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec19091",
   "metadata": {},
   "source": [
    "# Laboratorio 3\n",
    "\n",
    "\n",
    "## Integrantes\n",
    "- Gustavo Cruz 22779\n",
    "- Mathew Cordero 22982\n",
    "- Pedro Guzmán 22111\n",
    "\n",
    "## Repositorio\n",
    "\n",
    "[Link al Repositorio](https://github.com/donmatthiuz/ModelacionSimu/tree/lab3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3a01d",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio 1  \n",
    "\n",
    "Implementar los siguientes métodos de **descenso gradiente** (naïve = tamaño de paso α constante):  \n",
    "\n",
    "1. Descenso gradiente naïve con dirección de descenso **aleatoria**.  \n",
    "2. Descenso gradiente **máximo** naïve.  \n",
    "3. Descenso gradiente de **Newton**, con Hessiano exacto.  \n",
    "4. Un método de **gradiente conjugado** (Fletcher-Reeves, Hestenes-Stiefel o Polak-Ribiere).  \n",
    "5. El método **BFGS**.  \n",
    "\n",
    "### Requerimientos\n",
    "\n",
    "En cada uno de los métodos, la función debe recibir los siguientes argumentos:\n",
    "\n",
    "- La función objetivo `f`.  \n",
    "- El gradiente de la función objetivo `df`.  \n",
    "- El Hessiano `ddf` (cuando sea necesario).  \n",
    "- Un punto inicial `x0 ∈ R^n`.  \n",
    "- El tamaño de paso `α > 0`.  \n",
    "- El número máximo de iteraciones `maxIter`.  \n",
    "- La tolerancia `ε`.  \n",
    "- Un criterio de paro.  \n",
    "\n",
    "### Resultados esperados\n",
    "\n",
    "Los algoritmos deben devolver:  \n",
    "\n",
    "- La mejor solución encontrada `best` (la última de las aproximaciones calculadas).  \n",
    "- La secuencia de iteraciones `xk`.  \n",
    "- La secuencia de valores `f(xk)`.  \n",
    "- La secuencia de errores en cada paso (según el error de su criterio de paro).  \n",
    "\n",
    "Además, es deseable indicar:  \n",
    "\n",
    "- El número de iteraciones efectuadas por el algoritmo.  \n",
    "- Si se obtuvo o no la **convergencia** del método.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef26af",
   "metadata": {},
   "source": [
    "Lo primero que vamos a hacer es definir una clase con funciones que representan cada metodo|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dfe9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DescensoGradiente:\n",
    "    def __init__(self, F, df):\n",
    "        self.F = F     \n",
    "        self.df = df   \n",
    "\n",
    "    # Función para graficar resultados\n",
    "    def plot_results(self, fk, errors, metodo):\n",
    "        plt.figure(figsize=(12,5))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(fk, marker='o')\n",
    "        plt.title(f\"{metodo}: Evolución de F(x)\")\n",
    "        plt.xlabel(\"Iteración\")\n",
    "        plt.ylabel(\"F(x)\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(errors, marker='o', color='r')\n",
    "        plt.title(f\"{metodo}: Evolución del Error\")\n",
    "        plt.xlabel(\"Iteración\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Método de descenso aleatorio\n",
    "    def naive_random(self, x0 = np.zeros(2), alpha = 0.01, maxIter = 10000, tol = 1e-10):\n",
    "        xk = [x0]             \n",
    "        fk = [self.F(x0)]\n",
    "        errors = [np.linalg.norm(self.df(x0)) if self.df else np.inf]\n",
    "\n",
    "        x = np.array(x0, dtype=float)\n",
    "        for k in range(maxIter):\n",
    "            # Dirección aleatoria normalizada\n",
    "            d = np.random.randn(*x.shape)\n",
    "            d = d / np.linalg.norm(d)\n",
    "\n",
    "            x_new = x - alpha * d\n",
    "            f_new = self.F(x_new)\n",
    "\n",
    "            xk.append(x_new)\n",
    "            fk.append(f_new)\n",
    "            error = np.abs(fk[-1] - fk[-2])\n",
    "            errors.append(error)\n",
    "\n",
    "            if error < tol:\n",
    "                print(f\"Descenso Aleatorio: Convergió después de {k+1} iteraciones con error {error}\")\n",
    "                self.plot_results(fk, errors, \"Descenso Aleatorio\")\n",
    "                return x_new, xk, fk, errors, k+1, True\n",
    "\n",
    "            x = x_new\n",
    "\n",
    "        print(\"Descenso Aleatorio: No convergió\")\n",
    "        self.plot_results(fk, errors, \"Descenso Aleatorio\")\n",
    "        return x, xk, fk, errors, maxIter, False\n",
    "    \n",
    "    # Método de gradiente más pronunciado\n",
    "    def steepest_descent(self, x0 = np.zeros(2), alpha = 0.01, maxIter = 10000, tol = 1e-10): \n",
    "        xk = [x0]             \n",
    "        fk = [self.F(x0)]\n",
    "        errors = [np.linalg.norm(self.df(x0))]\n",
    "\n",
    "        x = np.array(x0, dtype=float)\n",
    "        for k in range(maxIter):\n",
    "            grad = self.df(x)\n",
    "            if np.linalg.norm(grad) < tol:\n",
    "                print(f\"Descenso por Gradiente: Convergió (gradiente < tol) después de {k} iteraciones\")\n",
    "                self.plot_results(fk, errors, \"Descenso por Gradiente\")\n",
    "                return x, xk, fk, errors, k, True\n",
    "\n",
    "            # Actualización del punto\n",
    "            x_new = x - alpha * grad\n",
    "            f_new = self.F(x_new)\n",
    "\n",
    "            xk.append(x_new)\n",
    "            fk.append(f_new)\n",
    "            error = np.abs(fk[-1] - fk[-2])\n",
    "            errors.append(error)\n",
    "\n",
    "            if error < tol:\n",
    "                print(f\"Descenso por Gradiente: Convergió (ΔF < tol) después de {k+1} iteraciones\")\n",
    "                self.plot_results(fk, errors, \"Descenso por Gradiente\")\n",
    "                return x_new, xk, fk, errors, k+1, True\n",
    "\n",
    "            x = x_new\n",
    "\n",
    "        print(\"Descenso por Gradiente: No convergió\")\n",
    "        self.plot_results(fk, errors, \"Descenso por Gradiente\")\n",
    "        return x, xk, fk, errors, maxIter, False\n",
    "\n",
    "    # Método de Newton\n",
    "    def newton_method(self, ddf, x0=np.zeros(2), maxIter=10000, tol=1e-10):\n",
    "        xk = [x0]\n",
    "        fk = [self.F(x0)]\n",
    "        errors = [np.linalg.norm(self.df(x0))]\n",
    "        x = np.array(x0, dtype=float)\n",
    "\n",
    "        for iteration in range(maxIter):\n",
    "            grad = self.df(x)\n",
    "            H = ddf(x)\n",
    "\n",
    "            if np.linalg.norm(grad) < tol:\n",
    "                print(f\"Método de Newton: Convergió (gradiente < tol) después de {iteration} iteraciones\")\n",
    "                self.plot_results(fk, errors, \"Método de Newton\")\n",
    "                return x, xk, fk, errors, iteration, True\n",
    "\n",
    "            try:\n",
    "                delta_x = np.linalg.solve(H, -grad)\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(f\"Método de Newton: Hessiano singular en iteración {iteration}\")\n",
    "                self.plot_results(fk, errors, \"Método de Newton\")\n",
    "                return x, xk, fk, errors, iteration, False\n",
    "\n",
    "            x_new = x + delta_x\n",
    "            f_new = self.F(x_new)\n",
    "\n",
    "            xk.append(x_new)\n",
    "            fk.append(f_new)\n",
    "            errors.append(np.linalg.norm(delta_x))\n",
    "\n",
    "            if np.linalg.norm(delta_x) < tol:\n",
    "                print(f\"Método de Newton: Convergió (Δx < tol) después de {iteration+1} iteraciones\")\n",
    "                self.plot_results(fk, errors, \"Método de Newton\")\n",
    "                return x_new, xk, fk, errors, iteration+1, True\n",
    "\n",
    "            x = x_new\n",
    "\n",
    "        print(\"Método de Newton: No convergió\")\n",
    "        self.plot_results(fk, errors, \"Método de Newton\")\n",
    "        return x, xk, fk, errors, maxIter, False\n",
    "\n",
    "    # Gradiente conjugado Fletcher-Reeves\n",
    "    def conjugate_gradient_flerev(self, x0=np.zeros(2), alpha=0.01, maxIter=1000, tol=1e-10):\n",
    "        xk = [np.array(x0, dtype=float)]\n",
    "        fk = [self.F(x0)]\n",
    "        errors = [np.linalg.norm(self.df(x0))]\n",
    "\n",
    "        x = np.array(x0, dtype=float)\n",
    "        grad = self.df(x)\n",
    "        d = -grad\n",
    "\n",
    "        for k in range(maxIter):\n",
    "            grad_old = grad.copy()\n",
    "\n",
    "            x_new = x + alpha * d\n",
    "            grad = self.df(x_new)\n",
    "\n",
    "            xk.append(x_new)\n",
    "            fk.append(self.F(x_new))\n",
    "            errors.append(np.linalg.norm(grad))\n",
    "\n",
    "            if np.linalg.norm(grad) < tol:\n",
    "                print(f\"Gradiente Conjugado (Fletcher-Reeves): Convergió después de {k+1} iteraciones\")\n",
    "                self.plot_results(fk, errors, \"Gradiente Conjugado (F-R)\")\n",
    "                return x_new, xk, fk, errors, k+1, True\n",
    "\n",
    "            beta = np.dot(grad, grad) / np.dot(grad_old, grad_old)\n",
    "            d = -grad + beta * d\n",
    "            x = x_new\n",
    "\n",
    "        print(\"Gradiente Conjugado (Fletcher-Reeves): No convergió\")\n",
    "        self.plot_results(fk, errors, \"Gradiente Conjugado (F-R)\")\n",
    "        return x, xk, fk, errors, maxIter, False\n",
    "\n",
    "    # Método BFGS\n",
    "    def bfgs_method(self, x0=np.zeros(2), tol=1e-6, maxIter=1000):\n",
    "        x = np.array(x0, dtype=float)\n",
    "        n = len(x)\n",
    "        H = np.eye(n)\n",
    "        xk = [x.copy()]\n",
    "        fk = [self.F(x)]\n",
    "        errors = [np.linalg.norm(self.df(x))]\n",
    "\n",
    "        for k in range(maxIter):\n",
    "            grad = self.df(x)\n",
    "            d = - H.dot(grad)\n",
    "            x_new = x + d\n",
    "            grad_new = self.df(x_new)\n",
    "\n",
    "            xk.append(x_new.copy())\n",
    "            fk.append(self.F(x_new))\n",
    "            errors.append(np.linalg.norm(grad_new))\n",
    "\n",
    "            if np.linalg.norm(x_new - x) < tol:\n",
    "                print(f\"BFGS: Convergió (Δx < tol) después de {k+1} iteraciones\")\n",
    "                self.plot_results(fk, errors, \"BFGS\")\n",
    "                return x_new, xk, fk, errors, k+1, True\n",
    "\n",
    "            s = x_new - x\n",
    "            y = grad_new - grad\n",
    "            rho = 1.0 / np.dot(y, s)\n",
    "            Hy = H.dot(y)\n",
    "            H += np.outer(s, s) * rho - np.outer(Hy, Hy) / np.dot(y, Hy)\n",
    "\n",
    "            x = x_new\n",
    "\n",
    "        print(\"BFGS: No convergió\")\n",
    "        self.plot_results(fk, errors, \"BFGS\")\n",
    "        return x, xk, fk, errors, maxIter, False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
