{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec19091",
   "metadata": {},
   "source": [
    "# Laboratorio 3\n",
    "\n",
    "\n",
    "## Integrantes\n",
    "- Gustavo Cruz 22779\n",
    "- Mathew Cordero 22982\n",
    "- Pedro Guzmán 22111\n",
    "\n",
    "## Repositorio\n",
    "\n",
    "[Link al Repositorio](https://github.com/donmatthiuz/ModelacionSimu/tree/lab3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3a01d",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio 1  \n",
    "\n",
    "Implementar los siguientes métodos de **descenso gradiente** (naïve = tamaño de paso α constante):  \n",
    "\n",
    "1. Descenso gradiente naïve con dirección de descenso **aleatoria**.  \n",
    "2. Descenso gradiente **máximo** naïve.  \n",
    "3. Descenso gradiente de **Newton**, con Hessiano exacto.  \n",
    "4. Un método de **gradiente conjugado** (Fletcher-Reeves, Hestenes-Stiefel o Polak-Ribiere).  \n",
    "5. El método **BFGS**.  \n",
    "\n",
    "### Requerimientos\n",
    "\n",
    "En cada uno de los métodos, la función debe recibir los siguientes argumentos:\n",
    "\n",
    "- La función objetivo `f`.  \n",
    "- El gradiente de la función objetivo `df`.  \n",
    "- El Hessiano `ddf` (cuando sea necesario).  \n",
    "- Un punto inicial `x0 ∈ R^n`.  \n",
    "- El tamaño de paso `α > 0`.  \n",
    "- El número máximo de iteraciones `maxIter`.  \n",
    "- La tolerancia `ε`.  \n",
    "- Un criterio de paro.  \n",
    "\n",
    "### Resultados esperados\n",
    "\n",
    "Los algoritmos deben devolver:  \n",
    "\n",
    "- La mejor solución encontrada `best` (la última de las aproximaciones calculadas).  \n",
    "- La secuencia de iteraciones `xk`.  \n",
    "- La secuencia de valores `f(xk)`.  \n",
    "- La secuencia de errores en cada paso (según el error de su criterio de paro).  \n",
    "\n",
    "Además, es deseable indicar:  \n",
    "\n",
    "- El número de iteraciones efectuadas por el algoritmo.  \n",
    "- Si se obtuvo o no la **convergencia** del método.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef26af",
   "metadata": {},
   "source": [
    "Lo primero que vamos a hacer es definir una clase con funciones que representan cada metodo|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfe9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class DescensoGradiente:\n",
    "    def __init__(self, F, df, ddf=None ):\n",
    "        self.F = F     \n",
    "        self.df = df   \n",
    "        self.ddf = ddf\n",
    "\n",
    "    def naive_random(self, x0, a, maxIter, tol=1e-6):\n",
    "        xk = [x0]             \n",
    "        fk = [self.F(x0)]         # valores f(xk)\n",
    "        errors = [np.linalg.norm(self.df(x0))] if self.df else [np.inf]\n",
    "\n",
    "        x = np.array(x0, dtype=float)\n",
    "        for k in range(maxIter):\n",
    "            # Generar dirección aleatoria normalizada\n",
    "            d = np.random.randn(*x.shape)\n",
    "            d = d / np.linalg.norm(d)\n",
    "\n",
    "            # Nuevo candidato\n",
    "            x_new = x - a * d\n",
    "            f_new = self.F(x_new)\n",
    "\n",
    "            # Guardar información\n",
    "            xk.append(x_new)\n",
    "            fk.append(f_new)\n",
    "            error = np.abs(fk[-1] - fk[-2])  # criterio basado en mejora\n",
    "            errors.append(error)\n",
    "\n",
    "            # Condiciones de paro\n",
    "            if error < tol:\n",
    "                return x_new, xk, fk, errors, k+1, True\n",
    "\n",
    "            # Actualizar\n",
    "            x = x_new\n",
    "\n",
    "        return x, xk, fk, errors, maxIter, False\n",
    "    \n",
    "    def steepest_descent(self, x0 = np.zeros(2), alpha = 0.01, maxIter = 10000, tol = 1e-10): \n",
    "        xk = [x0]             \n",
    "        fk = [self.F(x0)]     # valores f(xk)\n",
    "        errors = [np.linalg.norm(self.df(x0))]\n",
    "\n",
    "        x = np.array(x0, dtype=float)\n",
    "        for k in range(maxIter):\n",
    "            grad = self.df(x)\n",
    "\n",
    "            # Condición de paro: norma del gradiente\n",
    "            if np.linalg.norm(grad) < tol:\n",
    "                return x, xk, fk, errors, k, True\n",
    "\n",
    "            # Actualización este es el corzaon de nuestra funcion xk+1​=xk​−αdk​\n",
    "            x_new = x - alpha * grad\n",
    "            f_new = self.F(x_new)\n",
    "\n",
    "            # Guardar información\n",
    "            xk.append(x_new)\n",
    "            fk.append(f_new)\n",
    "            error = np.abs(fk[-1] - fk[-2]) \n",
    "            errors.append(error)\n",
    "\n",
    "            if error < tol:\n",
    "                return x_new, xk, fk, errors, k+1, True\n",
    "\n",
    "            x = x_new\n",
    "\n",
    "        return x, xk, fk, errors, maxIter, False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
