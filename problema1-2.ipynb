{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec19091",
   "metadata": {},
   "source": [
    "# Laboratorio 3\n",
    "\n",
    "\n",
    "## Integrantes\n",
    "- Gustavo Cruz 22779\n",
    "- Mathew Cordero 22982\n",
    "- Pedro Guzmán 22111\n",
    "\n",
    "## Repositorio\n",
    "\n",
    "[Link al Repositorio](https://github.com/donmatthiuz/ModelacionSimu/tree/lab3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3a01d",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio 1  \n",
    "\n",
    "Implementar los siguientes métodos de **descenso gradiente** (naïve = tamaño de paso α constante):  \n",
    "\n",
    "1. Descenso gradiente naïve con dirección de descenso **aleatoria**.  \n",
    "2. Descenso gradiente **máximo** naïve.  \n",
    "3. Descenso gradiente de **Newton**, con Hessiano exacto.  \n",
    "4. Un método de **gradiente conjugado** (Fletcher-Reeves, Hestenes-Stiefel o Polak-Ribiere).  \n",
    "5. El método **BFGS**.  \n",
    "\n",
    "### Requerimientos\n",
    "\n",
    "En cada uno de los métodos, la función debe recibir los siguientes argumentos:\n",
    "\n",
    "- La función objetivo `f`.  \n",
    "- El gradiente de la función objetivo `df`.  \n",
    "- El Hessiano `ddf` (cuando sea necesario).  \n",
    "- Un punto inicial `x0 ∈ R^n`.  \n",
    "- El tamaño de paso `α > 0`.  \n",
    "- El número máximo de iteraciones `maxIter`.  \n",
    "- La tolerancia `ε`.  \n",
    "- Un criterio de paro.  \n",
    "\n",
    "### Resultados esperados\n",
    "\n",
    "Los algoritmos deben devolver:  \n",
    "\n",
    "- La mejor solución encontrada `best` (la última de las aproximaciones calculadas).  \n",
    "- La secuencia de iteraciones `xk`.  \n",
    "- La secuencia de valores `f(xk)`.  \n",
    "- La secuencia de errores en cada paso (según el error de su criterio de paro).  \n",
    "\n",
    "Además, es deseable indicar:  \n",
    "\n",
    "- El número de iteraciones efectuadas por el algoritmo.  \n",
    "- Si se obtuvo o no la **convergencia** del método.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef26af",
   "metadata": {},
   "source": [
    "Lo primero que vamos a hacer es definir una clase con funciones que representan cada metodo|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfe9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def numerical_derivative(f, x, h=1e-6):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "class DescensoGradiente:\n",
    "    def __init__(self, F, df):\n",
    "        self.F = F     \n",
    "        self.df = df   \n",
    "\n",
    "    def naive_random(self, x0 = np.zeros(2), alpha = 0.01, maxIter = 10000, tol = 1e-10):\n",
    "        xk = [x0]             \n",
    "        fk = [self.F(x0)]         # valores f(xk)\n",
    "        errors = [np.linalg.norm(self.df(x0))] if self.df else [np.inf]\n",
    "\n",
    "        x = np.array(x0, dtype=float)\n",
    "        for k in range(maxIter):\n",
    "            # Generar dirección aleatoria normalizada\n",
    "            d = np.random.randn(*x.shape)\n",
    "            d = d / np.linalg.norm(d)\n",
    "\n",
    "            # Nuevo candidato\n",
    "            x_new = x - alpha * d\n",
    "            f_new = self.F(x_new)\n",
    "\n",
    "            # Guardar información\n",
    "            xk.append(x_new)\n",
    "            fk.append(f_new)\n",
    "            error = np.abs(fk[-1] - fk[-2])  # criterio basado en mejora\n",
    "            errors.append(error)\n",
    "\n",
    "            # Condiciones de paro\n",
    "            if error < tol:\n",
    "                return x_new, xk, fk, errors, k+1, True\n",
    "\n",
    "            # Actualizar\n",
    "            x = x_new\n",
    "\n",
    "        return x, xk, fk, errors, maxIter, False\n",
    "    \n",
    "    def steepest_descent(self, x0 = np.zeros(2), alpha = 0.01, maxIter = 10000, tol = 1e-10): \n",
    "        xk = [x0]             \n",
    "        fk = [self.F(x0)]     # valores f(xk)\n",
    "        errors = [np.linalg.norm(self.df(x0))]\n",
    "\n",
    "        x = np.array(x0, dtype=float)\n",
    "        for k in range(maxIter):\n",
    "            grad = self.df(x)\n",
    "\n",
    "            # Condición de paro: norma del gradiente\n",
    "            if np.linalg.norm(grad) < tol:\n",
    "                return x, xk, fk, errors, k, True\n",
    "\n",
    "            # Actualización este es el corzaon de nuestra funcion xk+1​=xk​−αdk​\n",
    "            x_new = x - alpha * grad\n",
    "            f_new = self.F(x_new)\n",
    "\n",
    "            # Guardar información\n",
    "            xk.append(x_new)\n",
    "            fk.append(f_new)\n",
    "            error = np.abs(fk[-1] - fk[-2]) \n",
    "            errors.append(error)\n",
    "\n",
    "            if error < tol:\n",
    "                return x_new, xk, fk, errors, k+1, True\n",
    "\n",
    "            x = x_new\n",
    "\n",
    "        return x, xk, fk, errors, maxIter, False\n",
    "    \n",
    "    def newton_method(self, ddf, x0=np.zeros(2), maxIter=10000, tol=1e-10):\n",
    "        xk = [x0]\n",
    "        fk = [self.F(x0)]\n",
    "        errors = [np.linalg.norm(self.df(x0))]\n",
    "        x = np.array(x0, dtype=float)\n",
    "\n",
    "        for iteration in range(maxIter):\n",
    "            grad = self.df(x)\n",
    "            H = ddf(x)  # Hessiano en x\n",
    "\n",
    "          \n",
    "            if np.linalg.norm(grad) < tol:\n",
    "                print(\"Converged (gradiente) after\", iteration, \"iterations.\")\n",
    "                return x, xk, fk, errors, iteration, True\n",
    "\n",
    "            # Resolver sistema lineal para delta_x\n",
    "            try:\n",
    "                delta_x = np.linalg.solve(H, -grad)\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"Hessian is singular!\")\n",
    "                return x, xk, fk, errors, iteration, False\n",
    "\n",
    "            x_new = x + delta_x\n",
    "            f_new = self.F(x_new)\n",
    "\n",
    "            # Guardar info\n",
    "            xk.append(x_new)\n",
    "            fk.append(f_new)\n",
    "            errors.append(np.linalg.norm(delta_x))\n",
    "\n",
    "            # Condición de paro\n",
    "            if np.linalg.norm(delta_x) < tol:\n",
    "                print(\"Converged (delta_x) after\", iteration + 1, \"iterations.\")\n",
    "                return x_new, xk, fk, errors, iteration + 1, True\n",
    "\n",
    "            x = x_new\n",
    "\n",
    "        return x, xk, fk, errors, maxIter, False\n",
    "\n",
    "    def conjugate_gradient_flerev(self, x0=np.zeros(2), alpha=0.01, maxIter=1000, tol=1e-10):\n",
    "        xk = [np.array(x0, dtype=float)]\n",
    "        fk = [self.F(x0)]\n",
    "        errors = [np.linalg.norm(self.df(x0))]\n",
    "\n",
    "        x = np.array(x0, dtype=float)\n",
    "        grad = self.df(x)\n",
    "        d = -grad  # primera dirección\n",
    "\n",
    "        for k in range(maxIter):\n",
    "            grad_old = grad.copy()\n",
    "\n",
    "            # Actualizar x\n",
    "            x_new = x + alpha * d\n",
    "            grad = self.df(x_new)\n",
    "\n",
    "            # Guardar info\n",
    "            xk.append(x_new)\n",
    "            fk.append(self.F(x_new))\n",
    "            errors.append(np.linalg.norm(grad))\n",
    "\n",
    "            # Condición de paro\n",
    "            if np.linalg.norm(grad) < tol:\n",
    "                return x_new, xk, fk, errors, k+1, True\n",
    "\n",
    "            # Calcular beta\n",
    "            beta = np.dot(grad, grad) / np.dot(grad_old, grad_old)\n",
    "            \n",
    "\n",
    "            # Actualizar dirección\n",
    "            d = -grad + beta * d\n",
    "            x = x_new\n",
    "\n",
    "        return x, xk, fk, errors, maxIter, False\n",
    "    \n",
    "    def bfgs_method(self, x0=np.zeros(2), tol=1e-6, maxIter=1000):\n",
    "        x = np.array(x0, dtype=float)\n",
    "        n = len(x)\n",
    "        H = np.eye(n)  # Aproximación inicial de Hessiano inverso\n",
    "        xk = [x.copy()]\n",
    "        fk = [self.F(x)]\n",
    "        errors = [np.linalg.norm(self.df(x))]\n",
    "\n",
    "        for k in range(maxIter):\n",
    "            grad = self.df(x)\n",
    "            # Dirección de búsqueda\n",
    "            d = - H.dot(grad)\n",
    "            x_new = x + d  # Actualización del punto\n",
    "            grad_new = self.df(x_new)\n",
    "\n",
    "            # Guardar información\n",
    "            xk.append(x_new.copy())\n",
    "            fk.append(self.F(x_new))\n",
    "            errors.append(np.linalg.norm(grad_new))\n",
    "\n",
    "            # Condición de paro\n",
    "            if np.linalg.norm(x_new - x) < tol:\n",
    "                return x_new, xk, fk, errors, k+1, True\n",
    "\n",
    "            # Actualizar Hessiano inverso con la fórmula BFGS\n",
    "            s = x_new - x\n",
    "            y = grad_new - grad\n",
    "            rho = 1.0 / np.dot(y, s)\n",
    "            Hy = H.dot(y)\n",
    "            H += np.outer(s, s) * rho - np.outer(Hy, Hy) / np.dot(y, Hy)\n",
    "\n",
    "            # Preparar siguiente iteración\n",
    "            x = x_new\n",
    "\n",
    "        return x, xk, fk, errors, maxIter, False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
